algorithm: PPO

training_steps: 20000
checkpoint_frequency: 20
policy_name: "0"

stop:
    #episode_reward_mean: 0
    training_iteration: 10
    #timesteps_total: 10000
    # time_total_s: 10000000

config:
    env: shape
    env_config:
        env_path: "./environments/shaping.x86_64"
        action_repeat: null
        show_window: true
        speedup: 1

    framework: torch

    lr: 0.0003
    lambda: 0.95
    gamma: 0.99

    vf_loss_coeff: 0.5
    vf_clip_param: .inf
    clip_param: 0.2
    entropy_coeff: 0.0001
    entropy_coeff_schedule: null
    grad_clip: 0.5
    use gae: False

    normalize_actions: False
    clip_actions: True

    rollout_fragment_length: 128
    sgd_minibatch_size: 128
    num_env_runners: 1
    num_envs_per_runner: 1
    train_batch_size: 128

    num_sgd_iter: 4
    batch_mode: truncate_episodes

    num_gpus: 0

    model:
        vf_share_layers: False
        fcnet_hiddens: [32, 32]
        use_lstm: True
        lstm_use_prev_action: False
        lstm_use_prev_reward: False
        max_seq_len: 64
        lstm_cell_size: 256

